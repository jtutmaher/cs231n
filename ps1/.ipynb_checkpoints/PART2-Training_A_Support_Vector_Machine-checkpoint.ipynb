{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#PLOT SETTINGS\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "#AUTO RELOAD MODULES\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# Renormalize\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.997094, grad total: -68.005682\n"
     ]
    }
   ],
   "source": [
    "from src.classifiers import *\n",
    "\n",
    "# Set SVM classifier\n",
    "classifier = svm(3073,10)\n",
    "W = np.random.rand(3073,10)*0.0001\n",
    "\n",
    "# Compute loss and gradient\n",
    "loss,grad = classifier.loss(W,X_dev,y_dev,0.00001)\n",
    "print 'loss: %f, grad total: %f' % (loss,np.min(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -26.284123 analytic: -26.284123, relative error: 8.749280e-13\n",
      "numerical: 3.774821 analytic: 3.774821, relative error: 1.169257e-11\n",
      "numerical: 36.956669 analytic: 36.956669, relative error: 1.784208e-13\n",
      "numerical: -8.500000 analytic: -8.500000, relative error: 3.254286e-12\n",
      "numerical: -18.789545 analytic: -18.789545, relative error: 1.855908e-12\n",
      "numerical: -27.112407 analytic: -27.112407, relative error: 7.647943e-13\n",
      "numerical: 17.737567 analytic: 17.737567, relative error: 1.723423e-12\n",
      "numerical: 58.935619 analytic: 58.935619, relative error: 7.806424e-14\n",
      "numerical: -19.491525 analytic: -19.491525, relative error: 1.912009e-12\n",
      "numerical: -16.112440 analytic: -16.112440, relative error: 2.553333e-12\n",
      "numerical: -35.595222 analytic: -35.595222, relative error: 1.713416e-12\n",
      "numerical: 16.596147 analytic: 16.596147, relative error: 1.573618e-12\n",
      "numerical: 16.343901 analytic: 16.343901, relative error: 3.331776e-12\n",
      "numerical: -6.711470 analytic: -6.711470, relative error: 5.332402e-12\n",
      "numerical: 12.480280 analytic: 12.480280, relative error: 1.416214e-12\n",
      "numerical: -16.448949 analytic: -16.448949, relative error: 3.203802e-12\n",
      "numerical: 6.981623 analytic: 6.981623, relative error: 3.637568e-12\n",
      "numerical: -0.483348 analytic: -0.483348, relative error: 5.231026e-11\n",
      "numerical: 36.320540 analytic: 36.320540, relative error: 2.209653e-13\n",
      "numerical: -2.680914 analytic: -2.680914, relative error: 8.903107e-12\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "loss, grad = classifier.loss(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should match\n",
    "# almost exactly along all dimensions.\n",
    "f = lambda w: classifier.loss(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# do the gradient check once again with regularization turned on\n",
    "# you didn't forget the regularization gradient did you?\n",
    "loss, grad = classifier.loss(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: classifier.loss(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "classifier = svm(X_train.shape[1],10)\n",
    "loss_hist = classifier.train(X_train,y_train,learning_rate=1e-7,reg=57142)\n",
    "plt.plot(loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
